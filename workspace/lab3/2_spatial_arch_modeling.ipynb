{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Spatial Architecture Modeling\n",
    "Now that you are familiar with the basic single PE setup, letâ€™s look at an example of a full system as shown in the figure below. This design is composed of two levels of on-chip storage -- the global buffer and the local scratchpads in each PE as described in part 1. Each datatype is sent via a network from the global buffer to the PE array, and there are inter-PE networks that are capable of sending various data types within the array. We provide you with the loop nest of a matmul on this design in the figure below. \n",
    "\n",
    "<br>\n",
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img align=\"left\" src=\"designs/system/figures/arch.png\" alt=\"Full System  Architecture Diagram\" style=\"margin:50px 0px 0px 50px; width:40%\">\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img  align=\"left\"  src=\"designs/system/figures/loopnest.png\" alt=\"System Loopnest\" style=\"width:50%\">\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "You are provided with a PE array that has 16 PEs. Assume you can design different architectures and associated mappings for every layer shape (i.e. both ```architecture.yaml``` and ```mapping.yaml``` can change across layer shapes). \n",
    "\n",
    "In specific, you can select the height and width of the PE array as long as the total number of PEs equal to 16, while keeping other architectural attributes the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================================================\n",
      "# Please do not modify this file. If there are double-curly-brace-enclosed\n",
      "# statements, they are placeholders that should be set from the notebooks.\n",
      "# ========================================================================\n",
      "architecture:\n",
      "  version: 0.4\n",
      "  nodes:\n",
      "  - !Container\n",
      "    name: system_arch\n",
      "    attributes:\n",
      "      # Top-level attributes inherited by all components unless overridden\n",
      "      technology: \"45nm\"\n",
      "      global_cycle_seconds: 1e-9\n",
      "      datawidth: 16\n",
      "\n",
      "  - !Component\n",
      "    name: DRAM                 # offchip DRAM is the source of all datatypes\n",
      "    class: DRAM                # assume DRAM is large enough to store all the data, so no depth specification needed\n",
      "    attributes:\n",
      "      width: 64                # width in bits\n",
      "      datawidth: datawidth\n",
      "\n",
      "  - !Container\n",
      "    name: chip\n",
      "    \n",
      "  - !Component\n",
      "    name: global_buffer\n",
      "    class: SRAM\n",
      "    attributes:\n",
      "      width: 128\n",
      "      depth: 2048\n",
      "      datawidth: datawidth\n",
      "      n_banks: 1\n",
      "      n_rdwr_ports: 2\n",
      "\n",
      "  - !Container\n",
      "    name: PE\n",
      "    spatial: {meshX: {{pe_meshX}}, meshY: {{pe_meshY}}}\n",
      "\n",
      "  - !Component\n",
      "    name: scratchpad\n",
      "    class: smart_storage  # definitions of the compound classes can be found under \"components\" folder\n",
      "    attributes: {depth: 128, width: 16, datawidth: datawidth}\n",
      "\n",
      "  # registers for the mac unit\n",
      "  - !Component\n",
      "    name: weight_reg\n",
      "    class: reg_storage\n",
      "    attributes: {depth: 1, width: 16, datawidth: datawidth}\n",
      "      \n",
      "  - !Component\n",
      "    name: input_activation_reg\n",
      "    class: reg_storage\n",
      "    attributes: {depth: 1, width: 16, datawidth: datawidth}\n",
      "\n",
      "  - !Component\n",
      "    name: output_activation_reg\n",
      "    class: reg_storage\n",
      "    attributes: {depth: 1, width: 16, datawidth: datawidth}\n",
      "\n",
      "  - !Component\n",
      "    name: mac\n",
      "    class: mac_compute\n",
      "    attributes: {num_pipline_stages: 2, datawidth: datawidth}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from loaders import *\n",
    "\n",
    "show_config('designs/system/arch.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1: What variable names change the number of PEs in the X and Y dimensions? Please give the name of the double-curly-brace-enclosed variables. Case sensitive.\n",
      "\t['pe_meshX', 'pe_meshY']\n"
     ]
    }
   ],
   "source": [
    "answer(\n",
    "    question='2.1',\n",
    "    subquestion='What variable names change the number of PEs in the X and Y dimensions? Please give the name of the double-curly-brace-enclosed variables. Case sensitive.',\n",
    "    answer= ['pe_meshX', 'pe_meshY'], # [First variable in curly braces, second variable in curly braces]\n",
    "    required_type=[str, str]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "With this spatial architecture, we will explore how the PE array shape impacts two metrics: utilization, which impacts throughput, and spatial data reuse, which impacts energy.\n",
    "\n",
    "We start with the workload and mapping below. The mapping has placeholder variables in double curly brackets that we will replace with numeric values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem:\n",
      "  version: 0.4\n",
      "  shape:\n",
      "    name: \"CNN_Layer\"\n",
      "    dimensions: [ C, M, R, S, N, P, Q ]\n",
      "    coefficients:\n",
      "    - name: Wstride\n",
      "      default: 1\n",
      "    - name: Hstride\n",
      "      default: 1\n",
      "    - name: Wdilation\n",
      "      default: 1\n",
      "    - name: Hdilation\n",
      "      default: 1\n",
      "\n",
      "    data_spaces:\n",
      "    - name: Weights\n",
      "      projection:\n",
      "      - [ [C] ]\n",
      "      - [ [M] ]\n",
      "      - [ [R] ]\n",
      "      - [ [S] ]\n",
      "    - name: Inputs\n",
      "      projection:\n",
      "      - [ [N] ]\n",
      "      - [ [C] ]\n",
      "      - [ [R, Wdilation], [P, Wstride] ] # SOP form: R*Wdilation + P*Wstride\n",
      "      - [ [S, Hdilation], [Q, Hstride] ] # SOP form: S*Hdilation + Q*Hstride\n",
      "    - name: Outputs\n",
      "      projection:\n",
      "      - [ [N] ]\n",
      "      - [ [M] ]\n",
      "      - [ [Q] ]\n",
      "      - [ [P] ]\n",
      "      read_write: True\n",
      "\n",
      "  instance:\n",
      "    C: 4  # inchn\n",
      "    M: 8  # outchn\n",
      "    R: 5   # filter height\n",
      "    S: 5   # filter width\n",
      "    P: 28  # ofmap height\n",
      "    Q: 28  # ofmap width\n",
      "    N: 50   # batch size\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_config('layer_shapes/conv2.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ========================================================================\n",
      "# Please do not modify this file. If there are double-curly-brace-enclosed\n",
      "# statements, they are placeholders that should be set from the notebooks.\n",
      "# ========================================================================\n",
      "mapping:\n",
      "- target: DRAM\n",
      "  type: temporal\n",
      "  factors: \n",
      "  - P=1\n",
      "  - Q=1\n",
      "  - R=1\n",
      "  - S=1\n",
      "  - N={{DRAM_factor_N}}\n",
      "  - M={{DRAM_factor_M}}\n",
      "  - C={{DRAM_factor_C}}\n",
      "  permutation: [S, R, Q, P, C, M, N] # don't change this\n",
      "\n",
      "- target: global_buffer\n",
      "  type: temporal\n",
      "  factors: \n",
      "  - P=1\n",
      "  - Q=1\n",
      "  - R=1\n",
      "  - S=1\n",
      "  - N={{global_buffer_factor_N}}\n",
      "  - M={{global_buffer_factor_M}}\n",
      "  - C={{global_buffer_factor_C}}\n",
      "  permutation: [S, R, Q, P, C, M, N] # don't change this\n",
      "\n",
      "- target: PE\n",
      "  type: spatial  # spatial constraint specification\n",
      "  factors: \n",
      "  - P=1\n",
      "  - Q=1\n",
      "  - R=1\n",
      "  - S=1\n",
      "  - N=1\n",
      "  - M={{PE_spatial_factor_M}}\n",
      "  - C={{PE_spatial_factor_C}}\n",
      "  permutation: [C, M, R, S, P, Q, N]\n",
      "  # tells at which index should the dimensions be mapped to Y (PE cols),\n",
      "  # the dimensions before that index all should map to X (PE rows)\n",
      "  split: 1\n",
      "\n",
      "- target: scratchpad\n",
      "  type: temporal\n",
      "  factors: \n",
      "  - R=0\n",
      "  - S=0\n",
      "  - P=0\n",
      "  - Q=0\n",
      "  - M=1\n",
      "  - C=1\n",
      "  - N={{scratchpad_factor_N}}\n",
      "  permutation: [Q, P, N, C, M, S, R]\n",
      "\n",
      "- target: scratchpad\n",
      "  type: dataspace\n",
      "  keep: [Weights]\n",
      "  bypass: [Inputs, Outputs]\n",
      "\n",
      "- target: weight_reg\n",
      "  type: dataspace\n",
      "  keep: [Weights]\n",
      "  bypass: [Inputs, Outputs]\n",
      "- target: weight_reg\n",
      "  type: temporal\n",
      "  factors: [R=1, S=1, P=1, Q=1, N=1, C=1, M=1]\n",
      "  permutation: [R, S, P, Q, C, M, N]\n",
      "- target: input_activation_reg\n",
      "  type: dataspace\n",
      "  keep: [Inputs]\n",
      "  bypass: [Weights, Outputs]\n",
      "- target: input_activation_reg\n",
      "  type: temporal\n",
      "  factors: [R=1, S=1, P=1, Q=1, N=1, C=1, M=1]\n",
      "  permutation: [R, S, P, Q, C, M, N]\n",
      "- target: output_activation_reg\n",
      "  type: dataspace\n",
      "  keep: [Outputs]\n",
      "  bypass: [Weights, Inputs]\n",
      "- target: output_activation_reg\n",
      "  type: temporal\n",
      "  factors: [R=1, S=1, P=1, Q=1, N=1, C=1, M=1]\n",
      "  permutation: [R, S, P, Q, C, M, N]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_config('designs/system/map.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(\n",
    "    question='2.2',\n",
    "    subquestion=f'Which rank (e.g., C, M, or P) is mapped to the X dimension of the PE array? Case sensitive.',\n",
    "    answer= 'C',\n",
    "    required_type=('C', 'M', 'N', 'R', 'S', 'P', 'Q')\n",
    ")\n",
    "answer(\n",
    "    question='2.2',\n",
    "    subquestion=f'Which rank (e.g., C, M, or P) is mapped to the Y dimension of the PE array? Case sensitive.',\n",
    "    answer= 'M',\n",
    "    required_type=('C', 'M', 'N', 'R', 'S', 'P', 'Q')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the rest of Part 2 (to the end of this notebook), we will assume a 1x16 PE array (the array shape is 1 in the X-dimension and 16 in the Y-dimension).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH_CONFIG = {'pe_meshX': 1, 'pe_meshY': 16}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "We will look at the impact of PE utilization on latency, and how PE utilization depends on the mapping. Inspect the following mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-04-02 20:57:49,394 - pytimeloop.accelergy_interface - Running Accelergy with command: accelergy /home/workspace/lab3/output_dir/parsed-processed-input.yaml -o ./output_dir/ -v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytimeloop.accelergy_interface:Running Accelergy with command: accelergy /home/workspace/lab3/output_dir/parsed-processed-input.yaml -o ./output_dir/ -v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAM [ Weights:800 (800) Inputs:204800 (204800) Outputs:313600 (313600) ] \n",
      "-------------------------------------------------------------------------\n",
      "| for N in [0:50)\n",
      "|   for M in [0:2)\n",
      "|     for C in [0:4)\n",
      "\n",
      "global_buffer [ Weights:100 (100) Inputs:1024 (1024) Outputs:3136 (3136) ] \n",
      "--------------------------------------------------------------------------\n",
      "|       for M in [0:4)\n",
      "\n",
      "inter_PE_spatial [ ] \n",
      "scratchpad [ Weights:25 (25) ] \n",
      "------------------------------\n",
      "|         for R in [0:5)\n",
      "|           for S in [0:5)\n",
      "|             for P in [0:28)\n",
      "|               for Q in [0:28)\n",
      "\n",
      "weight_reg [ Weights:1 (1) ] \n",
      "input_activation_reg [ Inputs:1 (1) ] \n",
      "output_activation_reg [ Outputs:1 (1) ] \n",
      "---------------------------------------\n",
      "|                 << Compute >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_example = dict( # Do not change this configuration!\n",
    "    DRAM_factor_N=50,\n",
    "    DRAM_factor_M=2,\n",
    "    DRAM_factor_C=4,\n",
    "    global_buffer_factor_N=1,\n",
    "    global_buffer_factor_M=4,\n",
    "    global_buffer_factor_C=1,\n",
    "    PE_spatial_factor_M=1,\n",
    "    PE_spatial_factor_C=1,\n",
    "    scratchpad_factor_N=1,\n",
    ")\n",
    "\n",
    "full_config = {\n",
    "    **config_example,\n",
    "    **ARCH_CONFIG\n",
    "}\n",
    "\n",
    "result = run_timeloop_model(\n",
    "    full_config,\n",
    "    architecture='designs/system/arch.yaml',\n",
    "    mapping='designs/system/map.yaml',\n",
    "    problem='layer_shapes/conv2.yaml'\n",
    ")\n",
    "stats = open('./output_dir/timeloop-model.stats.txt', 'r').read()\n",
    "mapping = result.mapping\n",
    "one_pe_latency = result.cycles\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3: What is the PE utilization (number of utilized PEs divided by total number of PEs)?.\n",
      "\t0.625\n"
     ]
    }
   ],
   "source": [
    "answer(\n",
    "    question='2.3',\n",
    "    subquestion=f'What is the PE utilization (number of utilized PEs divided by total number of PEs)?.',\n",
    "    answer= 0.625,\n",
    "    required_type=Number\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of this utilization, the mapping achieves the following latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31360000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_pe_latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following mapping, we map more of the M rank to the PE array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2025-04-02 21:36:06,433 - pytimeloop.accelergy_interface - Running Accelergy with command: accelergy /home/workspace/lab3/output_dir/parsed-processed-input.yaml -o ./output_dir/ -v\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytimeloop.accelergy_interface:Running Accelergy with command: accelergy /home/workspace/lab3/output_dir/parsed-processed-input.yaml -o ./output_dir/ -v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAM [ Weights:800 (800) Inputs:204800 (204800) Outputs:313600 (313600) ] \n",
      "-------------------------------------------------------------------------\n",
      "| for N in [0:50)\n",
      "|   for M in [0:2)\n",
      "|     for C in [0:4)\n",
      "\n",
      "global_buffer [ Weights:100 (100) Inputs:1024 (1024) Outputs:3136 (3136) ] \n",
      "inter_PE_spatial [ ] \n",
      "--------------------\n",
      "|       for M in [0:4) (Spatial-Y)\n",
      "\n",
      "scratchpad [ Weights:25 (25) ] \n",
      "------------------------------\n",
      "|         for R in [0:5)\n",
      "|           for S in [0:5)\n",
      "|             for P in [0:28)\n",
      "|               for Q in [0:28)\n",
      "\n",
      "weight_reg [ Weights:1 (1) ] \n",
      "input_activation_reg [ Inputs:1 (1) ] \n",
      "output_activation_reg [ Outputs:1 (1) ] \n",
      "---------------------------------------\n",
      "|                 << Compute >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config_example = dict( # Do not change this configuration!\n",
    "    DRAM_factor_N=50,\n",
    "    DRAM_factor_M=2,\n",
    "    DRAM_factor_C=4,\n",
    "    global_buffer_factor_N=1,\n",
    "    global_buffer_factor_M=1,\n",
    "    global_buffer_factor_C=1,\n",
    "    PE_spatial_factor_M=4,\n",
    "    PE_spatial_factor_C=1,\n",
    "    scratchpad_factor_N=1,\n",
    ")\n",
    "\n",
    "full_config = {\n",
    "    **config_example,\n",
    "    **ARCH_CONFIG\n",
    "}\n",
    "\n",
    "result = run_timeloop_model(\n",
    "    full_config,\n",
    "    architecture='designs/system/arch.yaml',\n",
    "    mapping='designs/system/map.yaml',\n",
    "    problem='layer_shapes/conv2.yaml'\n",
    ")\n",
    "stats = open('./output_dir/timeloop-model.stats.txt', 'r').read()\n",
    "mapping = result.mapping\n",
    "four_pe_latency = result.cycles\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3: What is the PE utilization (number of utilized PEs divided by total number of PEs)?.\n",
      "\t0.25\n"
     ]
    }
   ],
   "source": [
    "answer(\n",
    "    question='2.3',\n",
    "    subquestion=f'What is the PE utilization (number of utilized PEs divided by total number of PEs)?.',\n",
    "    answer= 0.25,\n",
    "    required_type=Number\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7840000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_pe_latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, this latency is 4.0 times lower.\n"
     ]
    }
   ],
   "source": [
    "print(f'As expected, this latency is {one_pe_latency/four_pe_latency} times lower.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that simply increasing the factor of the spatially mapped rank is not always possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(\n",
    "    question='2.3',\n",
    "    subquestion=f'What is the maximum factor of the spatially mapped C rank based on the PE array shape?',\n",
    "    answer= 'FILL ME',\n",
    "    required_type=int\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, even if we can increase the factor of the spatially mapped rank, it does not always result in higher utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer(\n",
    "    question='2.3',\n",
    "    subquestion=f'Assuming a larger factor of the spatial loop is possible given the PE array shape, increasing the factor will not increase PE utilization if the workload were _____-bound.',\n",
    "    answer= 'FILL ME',\n",
    "    required_type=('computation', 'memory bandwidth')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Now, we look at how spatial mapping affects spatial data reuse. Again, we use the mappings from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_example = dict( # Do not change this configuration!\n",
    "    DRAM_factor_N=50,\n",
    "    DRAM_factor_M=2,\n",
    "    DRAM_factor_C=4,\n",
    "    global_buffer_factor_N=1,\n",
    "    global_buffer_factor_M=4,\n",
    "    global_buffer_factor_C=1,\n",
    "    PE_spatial_factor_M=1,\n",
    "    PE_spatial_factor_C=1,\n",
    "    scratchpad_factor_N=1,\n",
    ")\n",
    "\n",
    "full_config = {\n",
    "    **config_example,\n",
    "    **ARCH_CONFIG\n",
    "}\n",
    "\n",
    "result = run_timeloop_model(\n",
    "    full_config,\n",
    "    architecture='designs/system/arch.yaml',\n",
    "    mapping='designs/system/map.yaml',\n",
    "    problem='layer_shapes/conv2.yaml'\n",
    ")\n",
    "stats = open('./output_dir/timeloop-model.stats.txt', 'r').read()\n",
    "mapping = result.mapping\n",
    "print(mapping)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now will find the number of reads from the `global_buffer` for the above mapping by tensor by looking through the outputted stats file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer by setting these variables\n",
    " one_pe_input_reads = # YOUR ANSWER HERE\n",
    " one_pe_weight_reads = # YOUR ANSWER HERE\n",
    " one_pe_output_reads = # YOUR ANSWER HERE\n",
    "answer(\n",
    "    question='2.4',\n",
    "    subquestion=f'How many input reads does each utilized PE fetch from the global_buffer?',\n",
    "    answer=one_pe_input_reads,\n",
    "    required_type=Number\n",
    ")\n",
    "answer(\n",
    "    question='2.4',\n",
    "    subquestion=f'How many weight reads does each utilized PE fetch from the global_buffer?',\n",
    "    answer=one_pe_weight_reads,\n",
    "    required_type=Number\n",
    ")\n",
    "answer(\n",
    "    question='2.4',\n",
    "\tsubquestion=f'How many output reads does each utilized PE fetch from the global_buffer?',\n",
    "\tanswer=one_pe_output_reads,\n",
    "\trequired_type=Number\n",
    ")\n",
    "\n",
    "manual_one_pe_global_buffer_reads = one_pe_input_reads + one_pe_weight_reads + one_pe_output_reads\n",
    "print('If you answered correctly, the following equality should hold.')\n",
    "print(f'Is {manual_one_pe_global_buffer_reads} == {62446400.0}? {manual_one_pe_global_buffer_reads == 62446400.0}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we increase the factor of M that is spatially mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_example = dict( # Do not change this configuration!\n",
    "    DRAM_factor_N=50,\n",
    "    DRAM_factor_M=2,\n",
    "    DRAM_factor_C=4,\n",
    "    global_buffer_factor_N=1,\n",
    "    global_buffer_factor_M=1,\n",
    "    global_buffer_factor_C=1,\n",
    "    PE_spatial_factor_M=4,\n",
    "    PE_spatial_factor_C=1,\n",
    "    scratchpad_factor_N=1,\n",
    ")\n",
    "\n",
    "full_config = {\n",
    "    **config_example,\n",
    "    **ARCH_CONFIG\n",
    "}\n",
    "\n",
    "result = run_timeloop_model(\n",
    "    full_config,\n",
    "    architecture='designs/system/arch.yaml',\n",
    "    mapping='designs/system/map.yaml',\n",
    "    problem='layer_shapes/conv2.yaml'\n",
    ")\n",
    "stats = open('./output_dir/timeloop-model.stats.txt', 'r').read()\n",
    "mapping = result.mapping\n",
    "print(mapping)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer by setting these variables\n",
    " four_pe_input_reads = # YOUR ANSWER HERE\n",
    " four_pe_weight_reads = # YOUR ANSWER HERE\n",
    " four_pe_output_reads = # YOUR ANSWER HERE\n",
    "answer(\n",
    "    question='2.4',\n",
    "    subquestion=f'How many input reads does each utilized PE fetch from the global_buffer now?',\n",
    "    answer= 'FILL ME',\n",
    "    required_type=Number\n",
    ")\n",
    "answer(\n",
    "    question='2.4',\n",
    "    subquestion=f'How many weight reads does each utilized PE fetch from the global_buffer now?',\n",
    "    answer= 'FILL ME',\n",
    "    required_type=Number\n",
    ")\n",
    "answer(\n",
    "    question='2.4',\n",
    "\tsubquestion=f'How many output reads does each utilized PE fetch from the global_buffer now?',\n",
    "\tanswer= 'FILL ME',\n",
    "\trequired_type=Number\n",
    ")\n",
    "\n",
    "manual_four_pe_global_buffer_reads = four_pe_input_reads + four_pe_weight_reads + four_pe_output_reads\n",
    "\n",
    "print('If you answered correctly, the following equality should hold.')\n",
    "print(f'Is {manual_four_pe_global_buffer_reads} == {38926400.0}? {manual_four_pe_global_buffer_reads == 38926400.0}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of input reads has decreased by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{one_pe_input_reads/four_pe_input_reads} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although since input reads are only a subset of total reads, the total reduction is only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{manual_one_pe_global_buffer_reads/manual_four_pe_global_buffer_reads} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because mapping the `M` rank spatially means that instead of reading the\n",
    "same input four times temporally, the input is read once from the global buffer\n",
    "and multicasted to the four PEs that will use it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
